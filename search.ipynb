{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a174cc4-c41e-401e-94da-f59cc38801e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T09:19:22.934594Z",
     "iopub.status.busy": "2023-11-25T09:19:22.934048Z",
     "iopub.status.idle": "2023-11-25T09:19:29.706274Z",
     "shell.execute_reply": "2023-11-25T09:19:29.705570Z",
     "shell.execute_reply.started": "2023-11-25T09:19:22.934566Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langdetect in /home/jupyter/.local/lib/python3.10/site-packages (1.0.9)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
      "Requirement already satisfied: sentence_transformers in /home/jupyter/.local/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: six in /kernel/lib/python3.10/site-packages (from langdetect) (1.16.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.6)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /home/jupyter/.local/lib/python3.10/site-packages (from sentence_transformers) (4.35.2)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.0.1+cu118)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.15.2+cu118)\n",
      "Requirement already satisfied: numpy in /home/jupyter/.local/lib/python3.10/site-packages (from sentence_transformers) (1.26.2)\n",
      "Requirement already satisfied: scikit-learn in /home/jupyter/.local/lib/python3.10/site-packages (from sentence_transformers) (1.3.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.10.1)\n",
      "Requirement already satisfied: sentencepiece in /home/jupyter/.local/lib/python3.10/site-packages (from sentence_transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /home/jupyter/.local/lib/python3.10/site-packages (from sentence_transformers) (0.19.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.27.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /kernel/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.8.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /kernel/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in /kernel/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (3.25.2)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (16.0.6)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/jupyter/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/jupyter/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /kernel/lib/python3.10/site-packages (from torchvision->sentence_transformers) (10.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /kernel/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n",
      "Collecting urllib3<1.27,>=1.21.1 (from requests->huggingface-hub>=0.4.0->sentence_transformers)\n",
      "  Downloading urllib3-1.26.18-py2.py3-none-any.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m615.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /kernel/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.7.22)\n",
      "Collecting charset-normalizer~=2.0.0 (from requests->huggingface-hub>=0.4.0->sentence_transformers)\n",
      "  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /kernel/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
      "Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: urllib3, charset-normalizer\n",
      "\u001b[33m  WARNING: The script normalizer is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.26.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed charset-normalizer-2.0.12 urllib3-1.26.18\n"
     ]
    }
   ],
   "source": [
    "%pip install langdetect nltk sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78665d3-7dfd-4d32-84e5-2ded4ffd9053",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Движок полнотекстового поиска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e0a65ea-0eda-4178-863d-aa74617cc198",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T09:19:29.707863Z",
     "iopub.status.busy": "2023-11-25T09:19:29.707476Z",
     "iopub.status.idle": "2023-11-25T09:19:34.377665Z",
     "shell.execute_reply": "2023-11-25T09:19:34.377007Z",
     "shell.execute_reply.started": "2023-11-25T09:19:29.707831Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: natasha in /home/jupyter/.local/lib/python3.10/site-packages (1.6.0)\n",
      "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.10/dist-packages (from natasha) (0.9.1)\n",
      "Requirement already satisfied: razdel>=0.5.0 in /home/jupyter/.local/lib/python3.10/site-packages (from natasha) (0.5.0)\n",
      "Requirement already satisfied: navec>=0.9.0 in /home/jupyter/.local/lib/python3.10/site-packages (from natasha) (0.10.0)\n",
      "Requirement already satisfied: slovnet>=0.6.0 in /home/jupyter/.local/lib/python3.10/site-packages (from natasha) (0.6.0)\n",
      "Requirement already satisfied: yargy>=0.16.0 in /home/jupyter/.local/lib/python3.10/site-packages (from natasha) (0.16.0)\n",
      "Requirement already satisfied: ipymarkup>=0.8.0 in /home/jupyter/.local/lib/python3.10/site-packages (from natasha) (0.9.0)\n",
      "Requirement already satisfied: intervaltree>=3 in /home/jupyter/.local/lib/python3.10/site-packages (from ipymarkup>=0.8.0->natasha) (3.1.0)\n",
      "Requirement already satisfied: numpy in /home/jupyter/.local/lib/python3.10/site-packages (from navec>=0.9.0->natasha) (1.26.2)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from pymorphy2->natasha) (0.7.2)\n",
      "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.10/dist-packages (from pymorphy2->natasha) (2.4.417127.4579844)\n",
      "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.10/dist-packages (from pymorphy2->natasha) (0.6.2)\n",
      "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from intervaltree>=3->ipymarkup>=0.8.0->natasha) (2.4.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install natasha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dbe0b969-fa96-41f8-a3e7-a0bb5260c253",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T10:24:50.229495Z",
     "iopub.status.busy": "2023-11-25T10:24:50.228644Z",
     "iopub.status.idle": "2023-11-25T10:24:50.247138Z",
     "shell.execute_reply": "2023-11-25T10:24:50.246516Z",
     "shell.execute_reply.started": "2023-11-25T10:24:50.229463Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from nltk import SnowballStemmer\n",
    "from tqdm import tqdm\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "class FullText:\n",
    "    def __init__(self, data):\n",
    "        self.idx = self.index_data(data)\n",
    "        \n",
    "\n",
    "    def get_stem(self, sentence: str) -> list[str]:\n",
    "        language = 'russian'\n",
    "        try:\n",
    "            language = 'english' if detect(sentence) != 'ru' else 'russian'\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        stemmer = SnowballStemmer(language)\n",
    "        my_words = sentence.split()\n",
    "        return [stemmer.stem(word) for word in my_words]\n",
    "\n",
    "    def index_data(self, data):\n",
    "        data_idx = {}\n",
    "        \n",
    "        for i in tqdm(range(0, 100000)):\n",
    "            try:\n",
    "                title = str(data.loc[i, 'video_title'])\n",
    "                video_id = data.loc[i, 'video_id']\n",
    "\n",
    "                stems = self.get_stem(title)\n",
    "                for stem in stems:\n",
    "                    if stem in data_idx:\n",
    "                        data_idx[stem].append(video_id)\n",
    "                    else:\n",
    "                        data_idx[stem] = [video_id]\n",
    "                            \n",
    "            except TypeError:\n",
    "                print(\"Huy\")\n",
    "                    \n",
    "        return data_idx\n",
    "\n",
    "    @staticmethod\n",
    "    def transliterate(name):\n",
    "        # Словарь с заменами\n",
    "        slovar_1 = {'ай': 'i', 'а': 'a', 'б': 'b', 'в': 'v', 'г': 'g', 'дж': \"j\", 'д': 'd', 'ей': 'ay', 'е': 'e',\n",
    "                    'ё': 'yo',\n",
    "                    'ж': 'j', 'з': 'z', 'и': 'i', 'й': 'i', 'к': 'k', 'л': 'l', 'м': 'm', 'н': 'n',\n",
    "                    'о': 'o', 'п': 'p', 'р': 'r', 'с': 's', 'т': 't', 'у': 'u', 'ф': 'f', 'х': 'h',\n",
    "                    'ц': 'c', 'ч': 'ch', 'ш': 'sh', 'щ': 'sch', 'ъ': '', 'ы': 'y', 'ь': '', 'э': 'e',\n",
    "                    'ю': 'u', 'я': 'ya'}\n",
    "\n",
    "        slovar_2 = {'ай': 'i', 'а': 'a', 'б': 'b', 'в': 'v', 'г': 'g', 'дж': \"j\", 'д': 'd', 'ей': 'ay', 'е': 'e',\n",
    "                    'ё': 'yo',\n",
    "                    'ж': 'j', 'з': 'z', 'и': 'i', 'й': 'i', 'к': 'c', 'л': 'l', 'м': 'm', 'н': 'n',\n",
    "                    'о': 'o', 'п': 'p', 'р': 'r', 'с': 's', 'т': 't', 'у': 'u', 'ф': 'f', 'х': 'h',\n",
    "                    'ц': 'c', 'ч': 'ch', 'ш': 'sh', 'щ': 'sch', 'ъ': '', 'ы': 'y', 'ь': '', 'э': 'e',\n",
    "                    'ю': 'u', 'я': 'ya'}\n",
    "\n",
    "        name = name.lower()\n",
    "\n",
    "        translit_1 = name\n",
    "        for key in slovar_1:\n",
    "            translit_1 = translit_1.replace(key, slovar_1[key])\n",
    "\n",
    "        translit_2 = name\n",
    "        for key in slovar_2:\n",
    "            translit_2 = translit_2.replace(key, slovar_2[key])\n",
    "\n",
    "        return [translit_1, translit_2]\n",
    "\n",
    "    def search(self, query):\n",
    "        stems = self.get_stem(query)\n",
    "        result = []\n",
    "        for stem in stems:\n",
    "            if stem in self.idx:\n",
    "                result += self.idx[stem]\n",
    "\n",
    "        for translit in self.transliterate(query) :\n",
    "            stems_t = self.get_stem(translit)\n",
    "            for stem in stems_t:\n",
    "                if stem in self.idx:\n",
    "                    result += self.idx[stem]\n",
    "\n",
    "        return set(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5483ba5-e0a1-4b36-b8a8-bb5832555ed8",
   "metadata": {},
   "source": [
    "# Движок семантического поиска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60fff622-aeea-415c-bb5b-1faceb37f165",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T09:19:34.722988Z",
     "iopub.status.busy": "2023-11-25T09:19:34.722422Z",
     "iopub.status.idle": "2023-11-25T09:19:37.021302Z",
     "shell.execute_reply": "2023-11-25T09:19:37.020704Z",
     "shell.execute_reply.started": "2023-11-25T09:19:34.722967Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sentence_transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "class Semantic:\n",
    "    def __init__(self, sentence_model: SentenceTransformer, threshold=0.5):\n",
    "        self.model = sentence_model\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def get_semantic_list(self, query: str, data: list[str]):\n",
    "        semantic_result = []\n",
    "\n",
    "        query_sentence = self.model.encode(query)\n",
    "        embeddings = self.model.encode(data)\n",
    "\n",
    "        i = 0\n",
    "\n",
    "        for sentence in embeddings:\n",
    "            s = sentence_transformers.util.cos_sim(a=sentence, b=query_sentence)\n",
    "            if s > self.threshold:\n",
    "                semantic_result.append(data[i])\n",
    "\n",
    "        return semantic_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74eda1c0-6a75-4918-a243-825608dc127b",
   "metadata": {},
   "source": [
    "# Парсинг данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b4a7e50f-7f81-4341-ac43-018e5d9a14be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T10:27:42.775978Z",
     "iopub.status.busy": "2023-11-25T10:27:42.775427Z",
     "iopub.status.idle": "2023-11-25T10:28:05.085428Z",
     "shell.execute_reply": "2023-11-25T10:28:05.084410Z",
     "shell.execute_reply.started": "2023-11-25T10:27:42.775955Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_parquet(\n",
    "    \"videos.parquet\",\n",
    "    engine=\"fastparquet\",\n",
    "    columns=[\n",
    "        \"video_id\",\n",
    "        \"video_title\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff555bba-4dfb-4345-a18b-cd6e0ff91d8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T09:19:58.889234Z",
     "iopub.status.busy": "2023-11-25T09:19:58.888167Z",
     "iopub.status.idle": "2023-11-25T09:19:58.903719Z",
     "shell.execute_reply": "2023-11-25T09:19:58.903191Z",
     "shell.execute_reply.started": "2023-11-25T09:19:58.889202Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34404561"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.axes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a567fc8-5858-49b8-b299-a57eff991f0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T09:21:15.557142Z",
     "iopub.status.busy": "2023-11-25T09:21:15.556587Z",
     "iopub.status.idle": "2023-11-25T09:30:18.095183Z",
     "shell.execute_reply": "2023-11-25T09:30:18.094442Z",
     "shell.execute_reply.started": "2023-11-25T09:21:15.557117Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [08:55<00:00, 186.86it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a12ee93eb8284256893105f2612bb5b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77c8a2aa0f6346b7b66af59e31bd9903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce381f3b8b3c4eab91b51e68352520d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/2.84k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eccd27ecf174c19872e4851096c592e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eef43e5977c4f9c860eb8cf233c80d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ace0465f2d0f411897edccc2809939e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/117M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b502430fa3ee403488375f9d6ef956d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/54.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad530c870388485fa76651d11e63cbbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac444b9129ce4b01a6f0645e5d2af7d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.41M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2476ccf3dfd049e3a4a9de03e16f270b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/410 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4cf7b5a53e64670b7a149dbaa79ebb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/1.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc0725a304ff40248a6f41a8854b1125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Пример использования\n",
    "import sentence_transformers\n",
    "\n",
    "def find_by_key(iterable, key, value):\n",
    "    for index, dict_ in enumerate(iterable):\n",
    "        if key in dict_ and dict_[key] == value:\n",
    "            return dict_\n",
    "\n",
    "\n",
    "fulltext = FullText(data)\n",
    "\n",
    "model = sentence_transformers.SentenceTransformer('inkoziev/sbert_pq' )\n",
    "semantic = Semantic(model, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63349632-58c9-4819-afdb-85f8e88770bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T09:40:51.266947Z",
     "iopub.status.busy": "2023-11-25T09:40:51.266457Z",
     "iopub.status.idle": "2023-11-25T09:41:37.971284Z",
     "shell.execute_reply": "2023-11-25T09:41:37.970160Z",
     "shell.execute_reply.started": "2023-11-25T09:40:51.266910Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Загрузка данных\n",
    "df = pd.read_parquet(\n",
    "    \"features.parquet\",\n",
    "    engine=\"fastparquet\",\n",
    "    columns=[\n",
    "        \"video_id\",\n",
    "        \"v_category\",\n",
    "        \"v_pub_datetime\",\n",
    "        \"v_likes\",\n",
    "        \"v_dislikes\",\n",
    "        \"total_comments\",\n",
    "        \"v_cr_click_long_view_7_days\"\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a790c5-da4e-4b70-a5c6-4101a22737b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Загрузка данных\n",
    "df = pd.read_parquet(\n",
    "    \"features.parquet\",\n",
    "    engine=\"fastparquet\",\n",
    "    columns=[\n",
    "        \"video_id\",\n",
    "        \"v_category\",\n",
    "        \"v_pub_datetime\",\n",
    "        \"v_likes\",\n",
    "        \"v_dislikes\",\n",
    "        \"total_comments\",\n",
    "        \"v_cr_click_long_view_7_days\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Функция для определения весов в зависимости от категории видео\n",
    "def get_weights(category):\n",
    "    if category in [\"Телепередачи\", \"Спорт\", \"Спорт/Игры\"]:\n",
    "        return {\"v_pub_datetime\": 0.4, \"v_dislikes\": 0.25, \"v_likes\": 0.2,  \"v_cr_click_long_view_7_days\": 0.1, \"total_comments\": 0.1}\n",
    "    else:\n",
    "        return {\"v_dislikes\": 0.25, \"v_likes\": 0.3, \"v_cr_click_long_view_7_days\": 0.2, \"total_comments\": 0.1, \"v_pub_datetime\": 0.1}\n",
    "\n",
    "# Определение весов для каждого фактора (важности)\n",
    "df[\"weights\"] = df[\"v_category\"].apply(get_weights)\n",
    "\n",
    "# Convert \"v_pub_datetime\" to numeric format\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"v_pub_datetime\"] = label_encoder.fit_transform(df[\"v_pub_datetime\"])\n",
    "\n",
    "# Нормализация данных\n",
    "scaler = StandardScaler()\n",
    "numeric_columns = [\"v_pub_datetime\", \"v_likes\", \"v_dislikes\", \"v_cr_click_long_view_7_days\", \"total_comments\", \"v_category\"]\n",
    "df[numeric_columns] = scaler.fit_transform(df[numeric_columns])\n",
    "\n",
    "# Разделение данных на обучающую и тестовую выборки\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Создание Pool для обучения\n",
    "train_pool = Pool(\n",
    "    train_data[numeric_columns],\n",
    "    label=train_data[\"video_id\"],\n",
    "    cat_features=[\"v_category\"]\n",
    ")\n",
    "\n",
    "# Инициализация модели CatBoost\n",
    "model = CatBoostRegressor(\n",
    "    iterations=1000, depth=6, learning_rate=0.1, loss_function=\"RMSE\"\n",
    ")\n",
    "\n",
    "# Обучение модели\n",
    "model.fit(train_pool, verbose=100)\n",
    "\n",
    "# Использование модели для ранжирования тестовых данных\n",
    "test_pool = Pool(\n",
    "    test_data[numeric_columns],\n",
    "    cat_features=[\"v_category\"]\n",
    ")\n",
    "test_data[\"predicted_rank\"] = model.predict(test_pool)\n",
    "\n",
    "# Сортировка тестовых данных по предсказанному рангу\n",
    "test_data = test_data.sort_values(by=\"predicted_rank\", ascending=False)\n",
    "print(test_data[[\"video_id\", \"predicted_rank\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb320a50-43ca-479a-94ff-7ec6d535e04c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T10:16:46.870347Z",
     "iopub.status.busy": "2023-11-25T10:16:46.869177Z",
     "iopub.status.idle": "2023-11-25T10:16:46.910084Z",
     "shell.execute_reply": "2023-11-25T10:16:46.909485Z",
     "shell.execute_reply.started": "2023-11-25T10:16:46.870285Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Битва сильнейших экстрасенсов 2023 смотреть | ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>битва сильнейших экстрасенсов 2023\\nбитва силь...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Экстрасенсы. Битва сильнейших, 4 выпуск</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>супер стар 4 сезон</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>пять ночей с Фредди</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>Шоу аватар 2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>Макс</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>смотреть мужское женское 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>шоу вована и лексуса</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>мужкое женское выпуск от 04 06 2021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  query\n",
       "0     Битва сильнейших экстрасенсов 2023 смотреть | ...\n",
       "1     битва сильнейших экстрасенсов 2023\\nбитва силь...\n",
       "2               Экстрасенсы. Битва сильнейших, 4 выпуск\n",
       "3                                    супер стар 4 сезон\n",
       "4                                   пять ночей с Фредди\n",
       "...                                                 ...\n",
       "1995                                    Шоу аватар 2023\n",
       "1996                                              Макс \n",
       "1997                      смотреть мужское женское 2019\n",
       "1998                               шоу вована и лексуса\n",
       "1999                мужкое женское выпуск от 04 06 2021\n",
       "\n",
       "[2000 rows x 1 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Получаем метрику\n",
    "queries = pd.read_csv(\"./test_dataset_submission_queries.csv\")\n",
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb395da-d693-4e1c-ad5f-b729b81fa2f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T10:39:27.303461Z",
     "iopub.status.busy": "2023-11-25T10:39:27.303061Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "resp_data = {\n",
    "    'query': [],\n",
    "    'video_id': [],\n",
    "}\n",
    "\n",
    "\n",
    "for index, row in tqdm(queries.iterrows()):\n",
    "    query = row[\"query\"]\n",
    "    results = fulltext.search(query)\n",
    "    \n",
    "    response = {\n",
    "        \"video_id\": [],\n",
    "        \"video_title\": []\n",
    "    }\n",
    "    \n",
    "    for res in results:\n",
    "        row = data[data['video_id'] == res]['video_title'].to_string(index=False)\n",
    "        response['video_id'].append(res)\n",
    "        response['video_title'].append(row)\n",
    "        \n",
    "    results = semantic.get_semantic_list(query, response['title'])\n",
    "    \n",
    "    data = pd.DataFrame(response)\n",
    "\n",
    "    for result in results:\n",
    "        video_id = data[data['video_title'] == result]['video_id'].to_string(index=False)\n",
    "        resp_data['query'].append(query)\n",
    "        resp_data['video_id'].append(video_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42afcc69-fbdd-40f2-9b41-ccea3b4b5a22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
