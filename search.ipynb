{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a174cc4-c41e-401e-94da-f59cc38801e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T03:31:51.879722Z",
     "iopub.status.busy": "2023-11-25T03:31:51.879308Z",
     "iopub.status.idle": "2023-11-25T03:31:59.753069Z",
     "shell.execute_reply": "2023-11-25T03:31:59.752390Z",
     "shell.execute_reply.started": "2023-11-25T03:31:51.879696Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langdetect in /home/jupyter/.local/lib/python3.10/site-packages (1.0.9)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
      "Requirement already satisfied: sentence_transformers in /home/jupyter/.local/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: six in /kernel/lib/python3.10/site-packages (from langdetect) (1.16.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.6)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /home/jupyter/.local/lib/python3.10/site-packages (from sentence_transformers) (4.35.2)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.0.1+cu118)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.15.2+cu118)\n",
      "Requirement already satisfied: numpy in /home/jupyter/.local/lib/python3.10/site-packages (from sentence_transformers) (1.26.2)\n",
      "Requirement already satisfied: scikit-learn in /home/jupyter/.local/lib/python3.10/site-packages (from sentence_transformers) (1.3.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.10.1)\n",
      "Requirement already satisfied: sentencepiece in /home/jupyter/.local/lib/python3.10/site-packages (from sentence_transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /home/jupyter/.local/lib/python3.10/site-packages (from sentence_transformers) (0.19.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.27.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /kernel/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.8.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /kernel/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in /kernel/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (3.25.2)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (16.0.6)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/jupyter/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/jupyter/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /kernel/lib/python3.10/site-packages (from torchvision->sentence_transformers) (10.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /kernel/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n",
      "Collecting urllib3<1.27,>=1.21.1 (from requests->huggingface-hub>=0.4.0->sentence_transformers)\n",
      "  Downloading urllib3-1.26.18-py2.py3-none-any.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m578.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /kernel/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.7.22)\n",
      "Collecting charset-normalizer~=2.0.0 (from requests->huggingface-hub>=0.4.0->sentence_transformers)\n",
      "  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /kernel/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
      "Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: urllib3, charset-normalizer\n",
      "\u001b[33m  WARNING: The script normalizer is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.26.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed charset-normalizer-2.0.12 urllib3-1.26.18\n"
     ]
    }
   ],
   "source": [
    "%pip install langdetect nltk sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78665d3-7dfd-4d32-84e5-2ded4ffd9053",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Движок полнотекстового поиска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e0a65ea-0eda-4178-863d-aa74617cc198",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T03:31:59.754605Z",
     "iopub.status.busy": "2023-11-25T03:31:59.754224Z",
     "iopub.status.idle": "2023-11-25T03:32:04.843943Z",
     "shell.execute_reply": "2023-11-25T03:32:04.843214Z",
     "shell.execute_reply.started": "2023-11-25T03:31:59.754581Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: natasha in /home/jupyter/.local/lib/python3.10/site-packages (1.6.0)\n",
      "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.10/dist-packages (from natasha) (0.9.1)\n",
      "Requirement already satisfied: razdel>=0.5.0 in /home/jupyter/.local/lib/python3.10/site-packages (from natasha) (0.5.0)\n",
      "Requirement already satisfied: navec>=0.9.0 in /home/jupyter/.local/lib/python3.10/site-packages (from natasha) (0.10.0)\n",
      "Requirement already satisfied: slovnet>=0.6.0 in /home/jupyter/.local/lib/python3.10/site-packages (from natasha) (0.6.0)\n",
      "Requirement already satisfied: yargy>=0.16.0 in /home/jupyter/.local/lib/python3.10/site-packages (from natasha) (0.16.0)\n",
      "Requirement already satisfied: ipymarkup>=0.8.0 in /home/jupyter/.local/lib/python3.10/site-packages (from natasha) (0.9.0)\n",
      "Requirement already satisfied: intervaltree>=3 in /home/jupyter/.local/lib/python3.10/site-packages (from ipymarkup>=0.8.0->natasha) (3.1.0)\n",
      "Requirement already satisfied: numpy in /home/jupyter/.local/lib/python3.10/site-packages (from navec>=0.9.0->natasha) (1.26.2)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from pymorphy2->natasha) (0.7.2)\n",
      "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.10/dist-packages (from pymorphy2->natasha) (2.4.417127.4579844)\n",
      "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.10/dist-packages (from pymorphy2->natasha) (0.6.2)\n",
      "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from intervaltree>=3->ipymarkup>=0.8.0->natasha) (2.4.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install natasha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbe0b969-fa96-41f8-a3e7-a0bb5260c253",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T03:32:04.846081Z",
     "iopub.status.busy": "2023-11-25T03:32:04.845508Z",
     "iopub.status.idle": "2023-11-25T03:32:06.842800Z",
     "shell.execute_reply": "2023-11-25T03:32:06.842180Z",
     "shell.execute_reply.started": "2023-11-25T03:32:04.846043Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from nltk import SnowballStemmer\n",
    "from tqdm import tqdm\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "class FullText:\n",
    "    def __init__(self, data):\n",
    "        self.idx = self.index_data(data)\n",
    "        \n",
    "\n",
    "    def get_stem(self, sentence: str) -> list[str]:\n",
    "        language = 'russian'\n",
    "        try:\n",
    "            language = 'english' if detect(sentence) != 'ru' else 'russian'\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        stemmer = SnowballStemmer(language)\n",
    "        my_words = sentence.split()\n",
    "        return [stemmer.stem(word) for word in my_words]\n",
    "\n",
    "    def index_data(self, data):\n",
    "        data_idx = {}\n",
    "        \n",
    "        for i in tqdm(range(0, 7000000)):\n",
    "            try:\n",
    "                title = str(data.loc[i, 'video_title'])\n",
    "                video_id = data.loc[i, 'video_id']\n",
    "\n",
    "                stems = self.get_stem(title)\n",
    "                for stem in stems:\n",
    "                    if stem in data_idx:\n",
    "                        data_idx[stem].append(video_id)\n",
    "                    else:\n",
    "                        data_idx[stem] = [video_id]\n",
    "                            \n",
    "            except TypeError:\n",
    "                print(\"Huy\")\n",
    "                    \n",
    "        return data_idx\n",
    "\n",
    "    @staticmethod\n",
    "    def transliterate(name):\n",
    "        # Словарь с заменами\n",
    "        slovar_1 = {'ай': 'i', 'а': 'a', 'б': 'b', 'в': 'v', 'г': 'g', 'дж': \"j\", 'д': 'd', 'ей': 'ay', 'е': 'e',\n",
    "                    'ё': 'yo',\n",
    "                    'ж': 'j', 'з': 'z', 'и': 'i', 'й': 'i', 'к': 'k', 'л': 'l', 'м': 'm', 'н': 'n',\n",
    "                    'о': 'o', 'п': 'p', 'р': 'r', 'с': 's', 'т': 't', 'у': 'u', 'ф': 'f', 'х': 'h',\n",
    "                    'ц': 'c', 'ч': 'ch', 'ш': 'sh', 'щ': 'sch', 'ъ': '', 'ы': 'y', 'ь': '', 'э': 'e',\n",
    "                    'ю': 'u', 'я': 'ya'}\n",
    "\n",
    "        slovar_2 = {'ай': 'i', 'а': 'a', 'б': 'b', 'в': 'v', 'г': 'g', 'дж': \"j\", 'д': 'd', 'ей': 'ay', 'е': 'e',\n",
    "                    'ё': 'yo',\n",
    "                    'ж': 'j', 'з': 'z', 'и': 'i', 'й': 'i', 'к': 'c', 'л': 'l', 'м': 'm', 'н': 'n',\n",
    "                    'о': 'o', 'п': 'p', 'р': 'r', 'с': 's', 'т': 't', 'у': 'u', 'ф': 'f', 'х': 'h',\n",
    "                    'ц': 'c', 'ч': 'ch', 'ш': 'sh', 'щ': 'sch', 'ъ': '', 'ы': 'y', 'ь': '', 'э': 'e',\n",
    "                    'ю': 'u', 'я': 'ya'}\n",
    "\n",
    "        name = name.lower()\n",
    "\n",
    "        translit_1 = name\n",
    "        for key in slovar_1:\n",
    "            translit_1 = translit_1.replace(key, slovar_1[key])\n",
    "\n",
    "        translit_2 = name\n",
    "        for key in slovar_2:\n",
    "            translit_2 = translit_2.replace(key, slovar_2[key])\n",
    "\n",
    "        return [translit_1, translit_2]\n",
    "\n",
    "    def search(self, query):\n",
    "        stems = self.get_stem(query)\n",
    "        result = []\n",
    "        for stem in stems:\n",
    "            if stem in self.idx:\n",
    "                result += self.idx[stem]\n",
    "\n",
    "        for translit in self.transliterate(query):\n",
    "            stems_t = self.get_stem(translit)\n",
    "            for stem in stems_t:\n",
    "                if stem in self.idx:\n",
    "                    result += self.idx[stem]\n",
    "\n",
    "        return set(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5483ba5-e0a1-4b36-b8a8-bb5832555ed8",
   "metadata": {},
   "source": [
    "# Движок семантического поиска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60fff622-aeea-415c-bb5b-1faceb37f165",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T03:32:06.844450Z",
     "iopub.status.busy": "2023-11-25T03:32:06.843851Z",
     "iopub.status.idle": "2023-11-25T03:32:16.037824Z",
     "shell.execute_reply": "2023-11-25T03:32:16.037168Z",
     "shell.execute_reply.started": "2023-11-25T03:32:06.844428Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sentence_transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "class Semantic:\n",
    "    def __init__(self, sentence_model: SentenceTransformer, threshold=0.5):\n",
    "        self.model = sentence_model\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def get_semantic_list(self, query: str, data: list[str]):\n",
    "        semantic_result = []\n",
    "\n",
    "        query_sentence = self.model.encode(query)\n",
    "        embeddings = self.model.encode(data)\n",
    "\n",
    "        i = 0\n",
    "\n",
    "        for sentence in embeddings:\n",
    "            s = sentence_transformers.util.cos_sim(a=sentence, b=query_sentence)\n",
    "            if s > self.threshold:\n",
    "                semantic_result.append(data[i])\n",
    "\n",
    "        return semantic_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74eda1c0-6a75-4918-a243-825608dc127b",
   "metadata": {},
   "source": [
    "# Парсинг данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4a7e50f-7f81-4341-ac43-018e5d9a14be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T03:32:16.039277Z",
     "iopub.status.busy": "2023-11-25T03:32:16.038732Z",
     "iopub.status.idle": "2023-11-25T03:33:02.041861Z",
     "shell.execute_reply": "2023-11-25T03:33:02.041024Z",
     "shell.execute_reply.started": "2023-11-25T03:32:16.039245Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_parquet(\n",
    "    \"videos.parquet\",\n",
    "    engine=\"fastparquet\",\n",
    "    columns=[\n",
    "        \"video_id\",\n",
    "        \"video_title\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff555bba-4dfb-4345-a18b-cd6e0ff91d8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T03:33:02.044310Z",
     "iopub.status.busy": "2023-11-25T03:33:02.043260Z",
     "iopub.status.idle": "2023-11-25T03:33:02.063481Z",
     "shell.execute_reply": "2023-11-25T03:33:02.062897Z",
     "shell.execute_reply.started": "2023-11-25T03:33:02.044268Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34404561"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.axes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a567fc8-5858-49b8-b299-a57eff991f0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T03:33:02.064691Z",
     "iopub.status.busy": "2023-11-25T03:33:02.064264Z",
     "iopub.status.idle": "2023-11-25T03:36:28.368611Z",
     "shell.execute_reply": "2023-11-25T03:36:28.367288Z",
     "shell.execute_reply.started": "2023-11-25T03:33:02.064661Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 35405/7000000 [03:25<11:14:19, 172.14it/s]\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/kernel/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3418, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-7-2178c6e9b86b>\", line 10, in <module>\n",
      "    fulltext = FullText(data)\n",
      "  File \"<ipython-input-3-b48e594a95c7>\", line 10, in __init__\n",
      "    self.idx = self.index_data(data)\n",
      "  File \"<ipython-input-3-b48e594a95c7>\", line 30, in index_data\n",
      "    video_id = data.loc[i, 'video_id']\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/pandas/core/indexing.py\", line 1145, in __getitem__\n",
      "    if self._is_scalar_access(key):\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/pandas/core/indexing.py\", line 1223, in _is_scalar_access\n",
      "    if len(key) != self.ndim:\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/kernel/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2045, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/kernel/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1170, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/kernel/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/kernel/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n",
      "    module = getmodule(object, filename)\n",
      "  File \"/usr/lib/python3.10/inspect.py\", line 875, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/usr/lib/python3.10/inspect.py\", line 844, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/usr/lib/python3.10/inspect.py\", line 826, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/usr/lib/python3.10/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-2178c6e9b86b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mfulltext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFullText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-b48e594a95c7>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-b48e594a95c7>\u001b[0m in \u001b[0;36mindex_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'video_title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 \u001b[0mvideo_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'video_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1144\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1145\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1146\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_takeable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_is_scalar_access\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;31m# b) provide a performant path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/kernel/lib/python3.10/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2044\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2045\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2046\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m/kernel/lib/python3.10/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2045\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2047\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2048\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kernel/lib/python3.10/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1434\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1436\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1437\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kernel/lib/python3.10/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1337\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m             )\n",
      "\u001b[0;32m/kernel/lib/python3.10/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1194\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kernel/lib/python3.10/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kernel/lib/python3.10/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "# Пример использования\n",
    "import sentence_transformers\n",
    "\n",
    "def find_by_key(iterable, key, value):\n",
    "    for index, dict_ in enumerate(iterable):\n",
    "        if key in dict_ and dict_[key] == value:\n",
    "            return dict_\n",
    "\n",
    "\n",
    "fulltext = FullText(data)\n",
    "\n",
    "model = sentence_transformers.SentenceTransformer('inkoziev/sbert_pq' )\n",
    "semantic = Semantic(model, 0.2)\n",
    "\n",
    "while True:\n",
    "    query = input(\"Query: \")\n",
    "    response = []\n",
    "\n",
    "    results = fulltext.search(query)\n",
    "    # поиск названия по id видео\n",
    "    for res in results:\n",
    "        response.append(find_by_key(data, \"video_id\", res)['video_title'])\n",
    "\n",
    "    results = semantic.get_semantic_list(query, response)\n",
    "    print(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a790c5-da4e-4b70-a5c6-4101a22737b0",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-25T03:36:28.369611Z",
     "iopub.status.idle": "2023-11-25T03:36:28.370067Z",
     "shell.execute_reply": "2023-11-25T03:36:28.369951Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Загрузка данных\n",
    "df = pd.read_parquet(\n",
    "    \"features.parquet\",\n",
    "    engine=\"fastparquet\",\n",
    "    columns=[\n",
    "        \"video_id\",\n",
    "        \"v_category\",\n",
    "        \"v_pub_datetime\",\n",
    "        \"v_likes\",\n",
    "        \"v_dislikes\",\n",
    "        \"v_comments\",\n",
    "        \"v_cr_click_long_view_7_days\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Функция для определения весов в зависимости от категории видео\n",
    "def get_weights(category):\n",
    "    if category in [\"Телепередачи\", \"Спорт\", \"Спорт/Игры\"]:\n",
    "        return {\"v_pub_datetime\": 0.4, \"v_dislikes\": 0.25, \"v_likes\": 0.2,  \"v_cr_click_long_view_7_days\": 0.1, \"v_comments\": 0.1}\n",
    "    else:\n",
    "        return {\"v_dislikes\": 0.25, \"v_likes\": 0.3, \"v_cr_click_long_view_7_days\": 0.2, \"v_comments\": 0.1, \"v_pub_datetime\": 0.1}\n",
    "\n",
    "# Добавление столбца с категорией видео\n",
    "df[\"category\"] = df[\"v_category\"]  # Замените \"category_column_name\" на имя вашего столбца с категорией\n",
    "\n",
    "# Определение весов для каждого фактора (важности)\n",
    "df[\"weights\"] = df[\"category\"].apply(get_weights)\n",
    "\n",
    "# Нормализация данных\n",
    "scaler = StandardScaler()\n",
    "df[[\"v_pub_datetime\", \"v_likes\", \"v_dislikes\", \"v_cr_click_long_view_7_days\", \"v_comments\"]] = scaler.fit_transform(\n",
    "    df[[\"v_pub_datetime\", \"v_likes\", \"v_dislikes\", \"v_cr_click_long_view_7_days\", \"v_comments\"]]\n",
    ")\n",
    "\n",
    "# Преобразование времени публикации в числовой формат\n",
    "df[\"v_pub_datetime\"] = pd.to_datetime(df[\"v_pub_datetime\"]).astype(int)\n",
    "\n",
    "# Разделение данных на обучающую и тестовую выборки\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Создание Pool для обучения\n",
    "train_pool = Pool(\n",
    "    train_data[[\"v_cr_click_long_view_7_days\", \"v_likes\", \"v_dislikes\", \"v_comments\", \"v_pub_datetime\"]],\n",
    "    label=train_data[\"label\"],\n",
    "    cat_features=[\"categorical_feature\"],\n",
    ")\n",
    "\n",
    "# Инициализация модели CatBoost\n",
    "model = CatBoostRegressor(\n",
    "    iterations=1000, depth=6, learning_rate=0.1, loss_function=\"RMSE\"\n",
    ")\n",
    "\n",
    "# Обучение модели\n",
    "model.fit(train_pool, verbose=100)\n",
    "\n",
    "# Использование модели для ранжирования тестовых данных\n",
    "test_pool = Pool(\n",
    "    test_data[[\"v_cr_click_long_view_7_days\", \"v_likes\", \"v_dislikes\", \"v_comments\", \"v_pub_datetime\"]],\n",
    "    cat_features=[\"categorical_feature\"],\n",
    ")\n",
    "test_data[\"predicted_rank\"] = model.predict(test_pool)\n",
    "\n",
    "# Сортировка тестовых данных по предсказанному рангу\n",
    "test_data = test_data.sort_values(by=\"predicted_rank\", ascending=False)\n",
    "print(test_data[[\"video_id\", \"predicted_rank\"]])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
